---
title: "Evolution Text Analysis"
author:
  - name: Jesse Powell
date: "'r Sys.Date()'"

---

```{r}
#first we need to import our data using readtext.

library(readtext)

evolution_data_txt <- readtext("~/Rexperiments/Evolution")

evolution_data_txt
```

```{r}
#We need to then convert our data to a corpus object
library(NLP)
library(tm)

evolution_corpus <- Corpus(VectorSource(evolution_data_txt$text))

evolution_corpus
```

```{r}
#Our corpus object needs to be preprocessed.

library(magrittr)

evolution_clean <- tm_map(evolution_corpus, content_transformer(tolower)) %>%
  tm_map(removeWords, c("the", "and", stopwords("english"))) %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)

evolution_clean
```

```{r}
#we need to now convert our corpus object to a dtm

evolution_dtm <- DocumentTermMatrix(evolution_clean)

evolution_dtm
```

```{r}
#we need to reduce the dimension of our dtm, which we will do by removing the less frequent terms.

evolution_reduced <- removeSparseTerms(evolution_dtm, 0.95)

evolution_reduced
```

```{r}
#We need to try and estimate the optimal number of topics for our topic model.

library(quanteda)
library(ldatuning)


result <- FindTopicsNumber(
  evolution_reduced,
  topics = seq(from = 350, to= 450, by = 10),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 23L,
  verbose = TRUE
)

FindTopicsNumber_plot(result)
```

```{r}
#Once we have identified an optimal k, we can build a topic model with k-many topics.
library(topicmodels)

evolution_model <- LDA(evolution_reduced, k = 400,  method = "Gibbs", control = list(seed = 77))

evolution_model
```

```{r}
#in the tidy format, we can explore the terms-per-topic and topics-per-document relations easier

library(tidytext)

evolution_topics <- tidy(evolution_model, matrix = "beta")

evolution_topics
```

```{r}
#we want to identify the top terms per topic, to try and get a summary of what each topic is about

library(ggplot2)
library(dplyr)

evolution_top_terms <- evolution_topics %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

evolution_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap (~ topic, scales = "free") +
  coord_flip()

evolution_top_terms
```

```{r}
#we can also try and get an idea of what each article is about by estimating what topics contribute most to each article
library(tidytext)

evolution_gamma <- tidy(evolution_model, matrix = "gamma")

evolution_gamma
```

```{r}
#this will let us visualize our results
evolution_top_gamma <- evolution_gamma %>%
  group_by(document) %>%
  top_n(1, gamma) %>%
  ungroup() %>%
  arrange(topic, -gamma)

evolution_top_gamma %>%
  mutate(document = reorder(document, gamma)) %>%
  ggplot(aes(document, gamma, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

evolution_top_gamma
```


```{r}
#we need to be able to visually explore our model; this function will allow us to do that

topicmodels_json_ldavis <- function(evolution_model, evolution_clean, evolution_reduced){
    # Required packages
    library(topicmodels)
    library(dplyr)
    library(stringi)
    library(tm)
    library(LDAvis)

    # Find required quantities
    phi <- posterior(evolution_model)$terms %>% as.matrix
    theta <- posterior(evolution_model)$topics %>% as.matrix
    vocab <- colnames(phi)
    doc_length <- vector()
    for (i in 1:length(evolution_clean)) {
        temp <- paste(evolution_clean[[i]]$content, collapse = ' ')
        doc_length <- c(doc_length, stri_count(temp, regex = '\\S+'))
    }
    temp_frequency <- as.matrix(evolution_reduced)
    freq_matrix <- data.frame(ST = colnames(temp_frequency),
                              Freq = colSums(temp_frequency))
    rm(temp_frequency)

    # Convert to json
    json_lda <- LDAvis::createJSON(phi = phi, theta = theta,
                            vocab = vocab,
                            doc.length = doc_length,
                            term.frequency = freq_matrix$Freq)

    return(json_lda)
}
```

```{r}
#applying the function
library(LDAvis)

evolution_json <- topicmodels_json_ldavis(evolution_model, evolution_clean, evolution_reduced)

serVis(evolution_json)
```


```{r}
#Thanks to the wonderful Marcelena Nagales for the help on this block.
library(dplyr)
library(xml2)


setwd("~/Rexperiments/XML")

evolution_xml <- list.files("~/Rexperiments/XML", pattern = ".xml$")

evolution_xml <- as.list(evolution_xml)
data  <- lapply(evolution_xml, function(x) {
# does the thing for one file (WITH PIPING)
# this reads first file (this could be the second one)
temp <- read_xml(x)
# long train of finding the childs of article
# we know that 3 levels is where pub-date is 
xml_children(temp) %>%
    xml_children() %>%
    xml_children() %>%
    xml_children() -> z
# determine names of last layer children
name <- xml_name(z)
# determines index of "year" in children of z
index <- which(name == "year")
#prints content of year node (yayyyyyy)
xml_contents(z[[index[[1]]]])
})

pubyear_int <- lapply(data, xml_integer)
```

```{r}
evolution_avg <- data.frame(year=integer(),
                 topic=integer(), 
                 average_gamma=double(), 
                 stringsAsFactors=FALSE) 
for (i in unique(evolution_df$year)) {
  for (j in unique(evolution_df$topic)) {
    
    indices <- which(evolution_df$year == i & evolution_df$topic == j )
    allGamma <- sum(evolution_df$gamma[indices])
    avg_gamma <- allGamma / length(indices)
    evolution_avg <- add_row(evolution_avg, 
                        year = i, 
                        topic = j, 
                        average_gamma = avg_gamma)


  }
}

```

```{r}
#We can now use our average values to create a stream graph.
#This will show us how the proportion of the discourse that is being dedicated
#to each topic, and their relative differences,
#is changing as a function of time.

library(dplyr)
library(streamgraph)

 evolution_streamgraph <- evolution_avg %>%
   streamgraph(key="topic", value="average_gamma", date="year", offset = "expand") %>%
   sg_fill_manual(evolution_colors_viridis) %>%
   sg_axis_x("year", "%y") %>%
   sg_legend(show = TRUE, label = "topic")

evolution_streamgraph
```

```{r}
gamma_mean <- aggregate(evolution_gamma22$gamma, list(year = evolution_gamma22$year), mean)

gamma_mean <- gamma_mean %>%
  rename(avg.gamma = x)


 
gamma_mean_plot <- ggplot(data = gamma_mean, mapping = aes(x = year, y = avg.gamma)) +
  geom_point() +
  labs(x = "year")

gamma_mean_plot
```

```{r}
#what we have is a time series; a series of gamma averages measured yearly for 60+ years. We should transform the data into a time series object to perform time series analysis on it.
library(forecast)

gamma_mean$year <- as.Date(gamma_mean$year, format = "%Y")
      
gamma_ts <- ts(gamma_mean$avg.gamma, start = c(1947), end = c(2015), frequency = 1)

#lagplot of the data. Effectively a test of randomness in the data. Should be no identifiable structure in the plots if random.
gglagplot(gamma_ts, set.lags =1:5, seasonal = FALSE)

#The autocorelation function tells us how correlated two measurements are seperated by a set number of measurements n. Autocorrelation gives us a measure of how correlated the measurements are with each other over different spans of time.
ggAcf(gamma_ts, lag.max = 24)

#right: autocorrelation plot of the data
ggAcf(gamma_ts, lag.max = 69)
```

```{r}
autoplot(gamma_ts) +
  geom_point() +
  geom_smooth() +
  labs( y = "avg.gamma", x = "year")
```

```{r}
#By removing the long term trend, we can see how the difference z[t] - z[t-1] is changing over time for all t and t-1 in our time series
autoplot(diff(gamma_ts)) +
  geom_smooth()
```

```{r}
drops <- c("document")

ts2 <- ts2[, !(names(ts2) %in% drops)] 

ts2
```

```{r}
gamma_plot_against <- ggplot(data = evolution_gamma22, mapping = aes(x = year, y = gamma)) +
  geom_point() +
  geom_line(data = (gamma_ts), col = "red")

gamma_plot_against
```
